# -*- coding: utf-8 -*-
"""Recommender_System_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rveHXL6VoKKqUby8jYMHEyAE4byCQC_F

<h2><h2 align = "middle"><b>Product Recommendation System Using Machine Learning</b></h2>

### **Initiation**

---

Installing Spark and JDK and Setting environment variables.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%sh
# apt-get install openjdk-8-jdk-headless -qq > /dev/null
# wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz
# tar -xvf spark-3.0.0-bin-hadoop3.2.tgz
# pip install -q findspark

import os
import pandas as pd
import json
#import gzip
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.0-bin-hadoop3.2"

"""Creating a local Spark Session."""

import findspark
findspark.init()
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql import functions as F
spark = SparkSession.builder.master("local[*]").getOrCreate()
sc = spark.sparkContext

"""Mounting on Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""### **Importing Libraries and Data**

---

"""

#from pyspark.sql.functions import *
from pyspark.sql.functions import isnan, when, count, col
from pyspark.sql.types import IntegerType, DoubleType
from pyspark.ml.feature import StringIndexer
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS
from pyspark.ml.tuning import ParamGridBuilder
from pyspark.ml.tuning import CrossValidator
# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
# from sklearn.neighbors import NearestNeighbors
# from sklearn.cluster import KMeans
# from sklearn.metrics import adjusted_rand_score
from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.sql import SparkSession, SQLContext
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml import Pipeline
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, StopWordsRemover
from pandas.core.reshape.concat import concat
from pyspark.ml.feature import PCA
import time
import pickle
import plotly.express as px
import pandas as pd

# Getting the data
wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Sports_and_Outdoors_5.json.gz

data = spark.read.json("/content/drive/Shared drives/IDS561/FinalProject/Amazon/350k/*.json")

"""### **Recommendation System Using Alternating Least Squares**

---

"""

df = data.select('asin', 'reviewerID', 'overall').withColumnRenamed("asin","product_id").withColumnRenamed("overall","rating")
df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()  # check for nulls

# Converting data type of rating to double type
df = df.withColumn("rating", df["rating"].cast(DoubleType()))

# Indexing reviewerID column into integer type
user_model = StringIndexer(inputCol="reviewerID", outputCol="reviewer_id_index").fit(df)
indexed = user_model.transform(df)
indexed_df = indexed.select(indexed["reviewer_id_index"].cast(IntegerType()).alias("reviewerID"), indexed["product_id"], indexed["rating"])

# Indexing product_id column into integer type
prod_model = StringIndexer(inputCol="product_id", outputCol="prod_id_index").fit(indexed_df)
p_indexed = prod_model.transform(indexed_df)
als_df = p_indexed.select(p_indexed["reviewerID"], p_indexed["prod_id_index"].cast(IntegerType()).alias("productID"), p_indexed["rating"])
als_df.show(5)

# Randomly split the data into train and test where 80% data is in train and remaining is test
train, test = als_df.randomSplit([0.8, 0.2])

# Define evaluator as RMSE
evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")

# Build the recommendation model using ALS on the training data
# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics
als = ALS(userCol="reviewerID", itemCol="productID", ratingCol="rating", nonnegative= True, coldStartStrategy='drop')
model = als.fit(train)

# Print the Root Mean Square Error of ALS Model
print("RMSE = ", evaluator.evaluate(model.transform(test)))

# Print default model parameters
print("Rank: ", model.rank)
print("Max Iter: ", model._java_obj.parent().getMaxIter())
print("Reg Param: ", model._java_obj.parent().getRegParam())

#Tuning model with Parameter Grid Builder
param_grid = ParamGridBuilder() \
     .addGrid(als.rank, [25]) \
     .addGrid(als.regParam, [.1]) \
     .addGrid(als.maxIter, [10]) \
     .build()
# #print(param_grid)

#Build a 10 fold cross validation
crossvalidation = CrossValidator(estimator = als, estimatorParamMaps = param_grid, evaluator = evaluator, numFolds=10)   

#Fit ALS model to training data
best_model = crossvalidation.fit(train).bestModel

#Calculate the RMSE on test data using the best set of parameters obtained after cross validation
print("RMSE value after cross validation is: ", evaluator.evaluate(best_model.transform(test)))

# Generate n Recommendations for all users
prod_rec = best_model.recommendForAllUsers(10)
#prod_rec.show()

recommend = prod_rec.toPandas()
recommend.head(5)

# Append user_id and product_id into a list and create a dataframe
users = []
recommendations = []
#For all data iterations
for i in range(len(recommend)):

  users.append(recommend.iloc[i,0])         #Add user_id to list
  user_recs = "" 

  for item in recommend.iloc[i,1]:          #Fetching only the item ID's from the recommendations
    user_recs = user_recs + ", " + str(item.asDict()['productID'])
  
  recommendations.append(user_recs[2:])     #Append the itemID's to a list

#Create a dataframe with the appended data
recommendations_df = pd.DataFrame(data = zip(users, recommendations), columns=["UserID", "ProductID"])

#Displaying users and product recommendations
#(top 10 products) for the first 10 users
recommendations_df.head(10)

#write to a text file
recommendations_df.to_csv('/content/drive/Shared drives/IDS561/FinalProject/ALSRecommendation_Output.csv', index=False)

"""### **Recommendation System Using K-Means Algorithm**

---

"""

df = data.select('asin', 'reviewText').dropna(how = "any")
# Data cleaning using regular expressions
df1 = df.toPandas()
df1['reviewText'] = df1['reviewText'].str.lower()
df1['reviewText'] = df1['reviewText'].replace({"'ll": " "}, regex=True)
df1 = df1.replace({"[^A-Za-z0-9 ]+": ""}, regex=True)
df = spark.createDataFrame(df1)

# Creating a Pipeline as an estimator 
tokenizer = Tokenizer(inputCol="reviewText", outputCol="words")
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=2000)
idf = IDF(inputCol="rawFeatures", outputCol="features")
kmeans = KMeans().setK(k).setSeed(1)
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, kmeans])

# Finding best k using Silhoutte Scores
wordsData = tokenizer.transform(df)
rem = remover.transform(wordsData)
featurizedData = hashingTF.transform(rem)
idfModel = idf.fit(featurizedData)
rescaledData = idfModel.transform(featurizedData)
t_df = rescaledData.select("asin", "features")
for k in range(3,17):
  t1 = time.time()
  kmeans = KMeans().setK(k).setSeed(1)
  model = kmeans.fit(t_df)
  predictions = model.transform(t_df)
  evaluator = ClusteringEvaluator()
  silhouette = evaluator.evaluate(predictions)
  t2 = time.time()
  print("Silhouette with squared euclidean distance with {} clusters = {} | Time Taken : {}".format(k,silhouette,(t2-t1)/60))

# Building k-means model with best k
kmeans = KMeans().setK(10).setSeed(1)
km = pipeline.fit(df)

# Transforming dataframe
preddata = km.transform(df)

# Plot clusters in best k using PCA
recc_pca = preddata.select('features', 'prediction')
pca = PCA(k=2,inputCol='features',outputCol='pcaFeatures')
pca_model = pca.fit(recc_pca)
result = pca_model.transform(recc_pca)
result_df = result.toPandas()
x = result_df['pcaFeatures'].apply(lambda x:x[0])
y = result_df['pcaFeatures'].apply(lambda x:x[1])
label = result_df['prediction']
data_df = pd.concat({'pca1':x,'pca2':y,'label':label},axis=1)
fig = px.scatter(data_df,x='pca1',y='pca2',color='label')
fig.show()

recc_df = preddata.select("asin", "prediction").toPandas()
recc_df = recc_df.groupby(['prediction'])['asin'].apply(lambda x: ','.join(x)).reset_index()
recc_df = recc_df.rename(columns={"prediction": "Cluster Number", "asin": "ProductID"})
recc_df.head()

# Recommend top 10 unique products for each searchword 
def search(a):
  lst = [(a, )]
  mod_df = spark.createDataFrame(lst, ['reviewText'])
  pred = km.transform(mod_df)
  output = pred.collect()[0]['prediction']
  fin = recc_df.loc[recc_df['Cluster'] == output]
  fin1 = fin['ProductID'].values[0]
  fin1 = fin1.split(',')
  return fin1

def unique(a):
  uni_list = []
  for i in a:
    if i not in uni_list:
      uni_list.append(i)
  return uni_list

unique(search("skateboard"))[:10]